import{_ as a,c as n,o as s,a5 as o,bY as l,bZ as r,b_ as i,b$ as t,c0 as p,c1 as h,c2 as d}from"./chunks/framework.k4KUTWql.js";const E=JSON.parse('{"title":"AI Large Model Integration Feature Installation","description":"","frontmatter":{},"headers":[],"relativePath":"docs/install-llm-plugin.md","filePath":"docs/install-llm-plugin.md","lastUpdated":1745565557000}'),c={name:"docs/install-llm-plugin.md"};function g(u,e,k,f,y,m){return s(),n("div",null,e[0]||(e[0]=[o(`<h1 id="ai-large-model-integration-feature-installation" tabindex="-1">AI Large Model Integration Feature Installation <a class="header-anchor" href="#ai-large-model-integration-feature-installation" aria-label="Permalink to “AI Large Model Integration Feature Installation”">​</a></h1><p>Here we introduce how to use the large model integration feature built into Liii STEM. This feature is exclusive to Liii STEM.</p><h2 id="installation" tabindex="-1">Installation <a class="header-anchor" href="#installation" aria-label="Permalink to “Installation”">​</a></h2><h3 id="supported-models" tabindex="-1">Supported Models <a class="header-anchor" href="#supported-models" aria-label="Permalink to “Supported Models”">​</a></h3><p>The currently supported models are:</p><ul><li><a href="https://www.DeepSeek.com/" target="_blank" rel="noreferrer">DeepSeek V3</a> (SiliconFlow, DeepSeek API)</li><li><a href="https://www.DeepSeek.com/" target="_blank" rel="noreferrer">DeepSeek R1</a> (SiliconFlow, DeepSeek API)</li><li><a href="https://github.com/QwenLM/Qwen" target="_blank" rel="noreferrer">Qwen-2.5 72B</a> (DeepSeek API)</li><li><a href="https://github.com/QwenLM/Qwen" target="_blank" rel="noreferrer">Qwen-2.5 7B</a> (DeepSeek API)</li></ul><h3 id="pandoc-for-macos-and-linux-only" tabindex="-1">Pandoc: For macOS and Linux Only <a class="header-anchor" href="#pandoc-for-macos-and-linux-only" aria-label="Permalink to “Pandoc: For macOS and Linux Only”">​</a></h3><p>Additionally, for <code>macOS</code> and <code>Linux</code> users, you need to install pandoc before use.</p><div class="language-zsh"><button title="Copy Code" class="copy"></button><span class="lang">zsh</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># macOS</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">brew</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> pandoc</span></span></code></pre></div><p>Or</p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Ubuntu</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> apt-get</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> pandoc</span></span></code></pre></div><p>After installing pandoc, you need to update the LLM plugin in Liii STEM, as shown below: <img src="`+l+'" alt="update_plugin"></p><p>Using Liii STEM&#39;s large model plugin also requires you to manually enter an API Key. We currently support SiliconFlow and DeepSeek APIs. Tutorials for obtaining APIs are available at <a href="#deepseek-api">How to get DeepSeek API</a> and <a href="#siliconflow-api">How to get SiliconFlow API</a>. The prices for each API are as follows, users need to purchase them themselves.</p><table tabindex="0"><thead><tr><th>Model Name</th><th>Context Length</th><th>Max Output Length</th><th>Input Price (Million Tokens)</th><th>Output Price (Million Tokens)</th><th>Key Capabilities &amp; Features</th><th>Remarks</th></tr></thead><tbody><tr><td><strong>DeepSeek-V3</strong></td><td>64K</td><td>8K</td><td>¥0.1 (cache hit)</td><td>¥2</td><td>High-performance general chat, code generation, logical reasoning, multilingual processing, cost-effective.</td><td>Input price is ¥1 when cache miss.</td></tr><tr><td><strong>DeepSeek-R1</strong></td><td>64K</td><td>8K</td><td>¥1 (cache hit)</td><td>¥16</td><td>Strong in math, code, natural language reasoning, supports model distillation, MIT license.</td><td>Input price is ¥4 when cache miss.</td></tr><tr><td><strong>Qwen-2.5 72B</strong></td><td>1M</td><td>8K</td><td>¥0.0036/kToken</td><td>¥4.13</td><td>72B parameters, supports long text processing, free for commercial use with &lt;100M MAU.</td><td>Provided by SiliconFlow. Offers various quantized versions (e.g., AWQ, GPTQ).</td></tr><tr><td><strong>Qwen-2.5 7B</strong></td><td>1M</td><td>8K</td><td>Free</td><td>Free</td><td>7B parameters, suitable for lightweight tasks, supports multi-GPU inference and quantized deployment.</td><td>Provided by SiliconFlow. Offers GGUF, AWQ, GPTQ, etc., suitable for resource-limited environments.</td></tr></tbody></table><p><a id="siliconflow-api"></a></p><h2 id="getting-siliconflow-api" tabindex="-1">Getting SiliconFlow API <a class="header-anchor" href="#getting-siliconflow-api" aria-label="Permalink to “Getting SiliconFlow API”">​</a></h2><p>Using Liii STEM&#39;s large model plugin requires you to manually enter an API Key. We currently support SiliconFlow and DeepSeek APIs. Below is the tutorial for obtaining the SiliconFlow API.</p><h3 id="step-1-apply-for-an-account-on-the-siliconflow-official-website" tabindex="-1">Step 1: Apply for an account on the SiliconFlow official website <a class="header-anchor" href="#step-1-apply-for-an-account-on-the-siliconflow-official-website" aria-label="Permalink to “Step 1: Apply for an account on the SiliconFlow official website”">​</a></h3><p>SiliconFlow official website: <a href="https://cloud.siliconflow.cn/i/h8qNv0VJ" target="_blank" rel="noreferrer">https://cloud.siliconflow.cn/i/h8qNv0VJ</a></p><p>Before use, you need to apply for an account on the SiliconFlow official website and obtain the corresponding API Key. Click to copy the key, as shown below:</p><p><img src="'+r+'" alt="api_key"></p><h3 id="step-2-open-liii-stem-and-configure-the-key" tabindex="-1">Step 2: Open Liii STEM and configure the key <a class="header-anchor" href="#step-2-open-liii-stem-and-configure-the-key" aria-label="Permalink to “Step 2: Open Liii STEM and configure the key”">​</a></h3><p>In Liii STEM, go to <code>Help</code> -&gt; <code>Plugins</code> -&gt; <code>LLM</code>, as shown below:</p><p><img src="'+i+'" alt="help_llm"></p><p>Then double-click to open the <code>llm_zh.tm</code> document.</p><p><img src="'+t+'" alt="llm_zh"></p><p>Paste your copied API Key into the position shown below, then click save.</p><p><img src="'+p+'" alt="insert_api_key"></p><p>After saving, click <code>Literate</code> -&gt; <code>Build buffer</code> in the menu. You can then enjoy our large model plugin!</p><p><a id="deepseek_api"></a></p><h2 id="getting-deepseek-api" tabindex="-1">Getting DeepSeek API <a class="header-anchor" href="#getting-deepseek-api" aria-label="Permalink to “Getting DeepSeek API”">​</a></h2><p>Using Liii STEM&#39;s large model plugin requires you to manually enter an API Key. We currently support SiliconFlow and DeepSeek APIs. Below is the tutorial for obtaining the DeepSeek API.</p><h3 id="step-1-apply-for-an-account-on-the-deepseek-official-website" tabindex="-1">Step 1: Apply for an account on the DeepSeek official website <a class="header-anchor" href="#step-1-apply-for-an-account-on-the-deepseek-official-website" aria-label="Permalink to “Step 1: Apply for an account on the DeepSeek official website”">​</a></h3><p>Before using <code>DeepSeek V3</code> and <code>R1</code>, you need to apply for an account on the <a href="https://platform.deepseek.com/api_keys" target="_blank" rel="noreferrer">DeepSeek official website</a> and obtain the corresponding API Key, as shown below.</p><p><img src="'+h+'" alt="deepseek_api"></p><h3 id="step-2-open-liii-stem-and-configure-the-key-1" tabindex="-1">Step 2: Open Liii STEM and configure the key <a class="header-anchor" href="#step-2-open-liii-stem-and-configure-the-key-1" aria-label="Permalink to “Step 2: Open Liii STEM and configure the key”">​</a></h3><p>In Liii STEM, go to <code>Help</code> -&gt; <code>Plugins</code> -&gt; <code>LLM</code>, as shown below: <img src="'+i+'" alt="help_llm"></p><p>Then open the <code>llm_zh.tm</code> document.</p><p><img src="'+t+'" alt="llm_zh"></p><p>Paste your copied API Key into the position shown below, then click save.</p><p><img src="'+d+`" alt="deepseek_api_key"></p><p>After saving, click <code>Literate</code> -&gt; <code>Build buffer</code> in the menu. You can then enjoy our large model plugin!</p><h2 id="proxy-configuration" tabindex="-1">Proxy Configuration <a class="header-anchor" href="#proxy-configuration" aria-label="Permalink to “Proxy Configuration”">​</a></h2><p>In some network environments, it might be necessary to access the model API service through a proxy server. Liii STEM&#39;s LLM plugin supports configuring independent proxies for different API providers.</p><h3 id="proxy-configuration-method" tabindex="-1">Proxy Configuration Method <a class="header-anchor" href="#proxy-configuration-method" aria-label="Permalink to “Proxy Configuration Method”">​</a></h3><p>Proxy configuration can be done by opening <code>Help</code> -&gt; <code>Plugins</code> -&gt; <code>LLM</code>, viewing the detailed help documentation, and configuring directly in the document.<br> Alternatively, it can be done in the <code>$HOME/.liii_llm_key.json</code> file. Each API provider can have separate proxy settings configured.</p><h4 id="basic-format" tabindex="-1">Basic Format <a class="header-anchor" href="#basic-format" aria-label="Permalink to “Basic Format”">​</a></h4><p>Proxy configuration uses the following format:</p><div class="language-json"><button title="Copy Code" class="copy"></button><span class="lang">json</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;&lt;Provider Domain&gt;&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: {</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  &quot;api-key&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;&lt;Your API Key&gt;&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  &quot;proxy&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: {</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    &quot;http&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;&lt;HTTP Proxy Address&gt;&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    &quot;https&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;&lt;HTTPS Proxy Address&gt;&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  }</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}</span></span></code></pre></div><h4 id="openai-proxy-configuration-example" tabindex="-1">OpenAI Proxy Configuration Example <a class="header-anchor" href="#openai-proxy-configuration-example" aria-label="Permalink to “OpenAI Proxy Configuration Example”">​</a></h4><p>Below is an example of configuring a proxy for OpenAI:</p><div class="language-json"><button title="Copy Code" class="copy"></button><span class="lang">json</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;openai.com&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: {</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  &quot;api-key&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Fill in your secret key here&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  &quot;proxy&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:{</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    &quot;http&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;http://127.0.0.1:7890&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    &quot;https&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;http://127.0.0.1:7890&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  }</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}</span></span></code></pre></div><h4 id="not-using-a-proxy" tabindex="-1">Not Using a Proxy <a class="header-anchor" href="#not-using-a-proxy" aria-label="Permalink to “Not Using a Proxy”">​</a></h4><p>If you do not need to use a proxy, there are two ways to handle it:</p><ol><li>Set &quot;proxy&quot; to an empty object:</li></ol><div class="language-json"><button title="Copy Code" class="copy"></button><span class="lang">json</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;proxy&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:{}</span></span></code></pre></div><ol start="2"><li>Completely remove the &quot;proxy&quot; field.</li></ol><h4 id="taking-effect" tabindex="-1">Taking Effect <a class="header-anchor" href="#taking-effect" aria-label="Permalink to “Taking Effect”">​</a></h4><p>If you modify the configuration in the help document, you need to:</p><ol><li>Click the menu item <code>Literate→Build buffer</code></li><li>Restart Liii STEM for the configuration to take effect.</li></ol><p>If you directly modify the <code>$HOME/.liii_llm_key.json</code> file:</p><ol><li>Save the file</li><li>Restart Liii STEM for the configuration to take effect.</li></ol><p><br><br><br></p><h3 id="common-proxy-configuration-issues-and-solutions" tabindex="-1">Common Proxy Configuration Issues and Solutions <a class="header-anchor" href="#common-proxy-configuration-issues-and-solutions" aria-label="Permalink to “Common Proxy Configuration Issues and Solutions”">​</a></h3><p>When configuring and using a proxy, if it doesn&#39;t work correctly, you might encounter the following common issues:</p><h4 id="_1-connection-timeout" tabindex="-1">1. Connection Timeout <a class="header-anchor" href="#_1-connection-timeout" aria-label="Permalink to “1. Connection Timeout”">​</a></h4><p><strong>Symptom</strong>: Long waiting time when requesting the API after configuring the proxy.</p><p><strong>Solution</strong>:</p><ul><li>Check if the proxy tool is running correctly.</li><li>Try changing the proxy server/line.</li><li>Confirm if the proxy address and port are correct.</li><li>Temporarily disable the firewall or security software to check if it&#39;s blocking the connection.</li></ul><br><h4 id="_2-error-during-use" tabindex="-1">2. Error During Use <a class="header-anchor" href="#_2-error-during-use" aria-label="Permalink to “2. Error During Use”">​</a></h4><p><strong>Symptom</strong>: Error occurs when using the large model after configuring the proxy.</p><p><strong>Solution</strong>:</p><ul><li>If the proxy you configured listens on the HTTPS protocol, e.g., <code>https://127.0.0.1:7890</code>, ensure the proxy server supports HTTPS requests.</li><li>Most local proxy tools (like Clash, V2RayN, Shadowsocks clients, etc.) expect HTTP or SOCKS protocol connections on their listening ports, not HTTPS connections.</li><li>Check the JSON format of the configuration file for errors, including commas, quotes, etc.</li></ul><p>Configuring a proxy is an important way to resolve network access restrictions, especially for services like OpenAI. If the above methods cannot solve your problem, you can contact us for more detailed help.</p>`,75)]))}const w=a(c,[["render",g]]);export{E as __pageData,w as default};
