import{_ as o,c as s,o as n,a5 as t,eW as l,eX as r,eY as i,eZ as a,e_ as p,e$ as h,f0 as c}from"./chunks/framework.CgTIqH8g.js";const E=JSON.parse('{"title":"Custom Model API (Professional Users)","description":"","frontmatter":{"title":"Custom Model API (Professional Users)"},"headers":[],"relativePath":"docs/install-llm-plugin.md","filePath":"docs/install-llm-plugin.md","lastUpdated":1761037894000}'),d={name:"docs/install-llm-plugin.md"};function u(g,e,k,f,y,m){return n(),s("div",null,[...e[0]||(e[0]=[t(`<h1 id="custom-model-api-professional-users" tabindex="-1">Custom Model API (Professional Users) <a class="header-anchor" href="#custom-model-api-professional-users" aria-label="Permalink to “Custom Model API (Professional Users)”">​</a></h1><p>Here we introduce how to use Liii STEM&#39;s built-in large language model integration functionality. This feature is exclusive to Liii STEM.</p><h2 id="installation" tabindex="-1">Installation <a class="header-anchor" href="#installation" aria-label="Permalink to “Installation”">​</a></h2><h3 id="supported-models" tabindex="-1">Supported Models <a class="header-anchor" href="#supported-models" aria-label="Permalink to “Supported Models”">​</a></h3><p>Currently supported models include:</p><ul><li><a href="https://www.DeepSeek.com/" target="_blank" rel="noreferrer">DeepSeek V3</a> (SiliconFlow, DeepSeek API)</li><li><a href="https://www.DeepSeek.com/" target="_blank" rel="noreferrer">DeepSeek R1</a> (SiliconFlow, DeepSeek API)</li><li><a href="https://github.com/QwenLM/Qwen" target="_blank" rel="noreferrer">Qwen-2.5 72B</a> (DeepSeek API)</li><li><a href="https://github.com/QwenLM/Qwen" target="_blank" rel="noreferrer">Qwen-2.5 7B</a> (DeepSeek API)</li></ul><h3 id="pandoc-linux-only" tabindex="-1">Pandoc: Linux Only <a class="header-anchor" href="#pandoc-linux-only" aria-label="Permalink to “Pandoc: Linux Only”">​</a></h3><p>Additionally, for <code>Linux</code> users, you need to install pandoc before use:</p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Ubuntu</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> apt-get</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> pandoc</span></span></code></pre></div><p>After installing pandoc, you need to update the LLM plugin in Liii STEM, as shown below: <img src="`+l+'" alt="Update Plugin"></p><p>We currently have SiliconFlow and DeepSeek APIs built-in, so you can use our large language model functionality without any configuration. For specific usage, please refer to <a href="./guide-llm.html">Built-in Large Language Model Usage Tutorial</a></p><p>We also support manually entering your own API Key. Tutorials for obtaining APIs are in <a href="#deepseek-api">How to Get DeepSeek API</a> and <a href="#siliconflow-api">How to Get SiliconFlow API</a>.</p><p><a id="siliconflow-api"></a></p><h2 id="siliconflow-api-acquisition" tabindex="-1">SiliconFlow API Acquisition <a class="header-anchor" href="#siliconflow-api-acquisition" aria-label="Permalink to “SiliconFlow API Acquisition”">​</a></h2><p>Using Liii STEM&#39;s large language model plugin requires you to manually enter API Key. We currently support SiliconFlow and DeepSeek APIs. Below is the tutorial for obtaining SiliconFlow API.</p><h3 id="step-1-apply-for-account-on-siliconflow-official-website" tabindex="-1">Step 1: Apply for Account on SiliconFlow Official Website <a class="header-anchor" href="#step-1-apply-for-account-on-siliconflow-official-website" aria-label="Permalink to “Step 1: Apply for Account on SiliconFlow Official Website”">​</a></h3><p>SiliconFlow Official Website: <a href="https://cloud.siliconflow.cn/i/h8qNv0VJ" target="_blank" rel="noreferrer">https://cloud.siliconflow.cn/i/h8qNv0VJ</a></p><p>Before use, you need to apply for an account on the SiliconFlow official website and obtain the corresponding API Key. Click to copy the key. As shown below:</p><p><img src="'+r+'" alt="API Key"></p><h3 id="step-2-open-liii-stem-to-configure-key" tabindex="-1">Step 2: Open Liii STEM to Configure Key <a class="header-anchor" href="#step-2-open-liii-stem-to-configure-key" aria-label="Permalink to “Step 2: Open Liii STEM to Configure Key”">​</a></h3><p>In Liii STEM, open <code>Help</code> -&gt; <code>Plugins</code> -&gt; <code>LLM</code>, as shown below:</p><p><img src="'+i+'" alt="Help LLM"></p><p>Then double-click to open the <code>llm_zh.tm</code> document</p><p><img src="'+a+'" alt="LLM Chinese"></p><p>Paste your copied API Key to the position shown below, then click Save.</p><p><img src="'+p+'" alt="Insert API Key"></p><p>After saving, click <code>Literate</code> -&gt; <code>Build buffer</code> in the menu, then you can enjoy our large language model plugin!</p><p><a id="deepseek_api"></a></p><h2 id="deepseek-api-acquisition" tabindex="-1">DeepSeek API Acquisition <a class="header-anchor" href="#deepseek-api-acquisition" aria-label="Permalink to “DeepSeek API Acquisition”">​</a></h2><p>Using Liii STEM&#39;s large language model plugin requires you to manually enter API Key. We currently support SiliconFlow and DeepSeek APIs. Below is the tutorial for obtaining DeepSeek API.</p><h3 id="step-1-apply-for-account-on-deepseek-official-website" tabindex="-1">Step 1: Apply for Account on DeepSeek Official Website <a class="header-anchor" href="#step-1-apply-for-account-on-deepseek-official-website" aria-label="Permalink to “Step 1: Apply for Account on DeepSeek Official Website”">​</a></h3><p>Before using <code>DeepSeek V3</code> and <code>R1</code>, you need to apply for an account on <a href="https://platform.deepseek.com/api_keys" target="_blank" rel="noreferrer">DeepSeek&#39;s official website</a> and obtain the corresponding API Key. As shown below.</p><p><img src="'+h+'" alt="DeepSeek API"></p><h3 id="step-2-open-liii-stem-to-configure-key-1" tabindex="-1">Step 2: Open Liii STEM to Configure Key <a class="header-anchor" href="#step-2-open-liii-stem-to-configure-key-1" aria-label="Permalink to “Step 2: Open Liii STEM to Configure Key”">​</a></h3><p>In Liii STEM, open <code>Help</code> -&gt; <code>Plugins</code> -&gt; <code>LLM</code>, as shown below <img src="'+i+'" alt="Help LLM"></p><p>Then open the <code>llm_zh.tm</code> document</p><p><img src="'+a+'" alt="LLM Chinese"></p><p>Paste your copied API Key to the position shown below, then click Save.</p><p><img src="'+c+`" alt="DeepSeek API Key"></p><p>After saving, click <code>Literate</code> -&gt; <code>Build buffer</code> in the menu, then you can enjoy our large language model plugin!</p><h2 id="proxy-configuration" tabindex="-1">Proxy Configuration <a class="header-anchor" href="#proxy-configuration" aria-label="Permalink to “Proxy Configuration”">​</a></h2><p>In certain network environments, it may be necessary to access model API services through proxy servers. Liii STEM&#39;s LLM plugin supports configuring independent proxies for different API providers.</p><h3 id="proxy-configuration-methods" tabindex="-1">Proxy Configuration Methods <a class="header-anchor" href="#proxy-configuration-methods" aria-label="Permalink to “Proxy Configuration Methods”">​</a></h3><p>Proxy configuration can be done by opening <code>Help</code> -&gt; <code>Plugins</code> -&gt; <code>LLM</code>, viewing the detailed help documentation, and configuring directly in the document.<br> It can also be done in the <code>$HOME/.liii_llm_key.json</code> file. Each API provider can be configured with separate proxy settings.</p><h4 id="basic-format" tabindex="-1">Basic Format <a class="header-anchor" href="#basic-format" aria-label="Permalink to “Basic Format”">​</a></h4><p>Proxy configuration uses the following format:</p><div class="language-json"><button title="Copy Code" class="copy"></button><span class="lang">json</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;&lt;provider domain&gt;&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: {</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  &quot;api-key&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;&lt;your API key&gt;&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  &quot;proxy&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: {</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    &quot;http&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;&lt;HTTP proxy address&gt;&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    &quot;https&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;&lt;HTTPS proxy address&gt;&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  }</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}</span></span></code></pre></div><h4 id="openai-proxy-configuration-example" tabindex="-1">OpenAI Proxy Configuration Example <a class="header-anchor" href="#openai-proxy-configuration-example" aria-label="Permalink to “OpenAI Proxy Configuration Example”">​</a></h4><p>Below is an example of configuring proxy for OpenAI:</p><div class="language-json"><button title="Copy Code" class="copy"></button><span class="lang">json</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;openai.com&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: {</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  &quot;api-key&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Fill in your secret key here&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  &quot;proxy&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:{</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    &quot;http&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;http://127.0.0.1:7890&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    &quot;https&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;http://127.0.0.1:7890&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  }</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}</span></span></code></pre></div><h4 id="not-using-proxy" tabindex="-1">Not Using Proxy <a class="header-anchor" href="#not-using-proxy" aria-label="Permalink to “Not Using Proxy”">​</a></h4><p>If you don&#39;t need to use proxy, there are two ways to handle it:</p><ol><li>Set &quot;proxy&quot; to empty object:</li></ol><div class="language-json"><button title="Copy Code" class="copy"></button><span class="lang">json</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;proxy&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:{}</span></span></code></pre></div><ol start="2"><li>Completely delete the &quot;proxy&quot; field</li></ol><h4 id="activation-methods" tabindex="-1">Activation Methods <a class="header-anchor" href="#activation-methods" aria-label="Permalink to “Activation Methods”">​</a></h4><p>If you modify configuration in the help documentation, you need to:</p><ol><li>Click menu item <code>Literate→Build buffer</code></li><li>Restart Liii STEM to make configuration effective</li></ol><p>If you directly modify the <code>$HOME/.liii_llm_key.json</code> file:</p><ol><li>Save the file</li><li>Restart Liii STEM to make configuration effective</li></ol><p><br><br><br></p><h3 id="common-proxy-configuration-issues-and-solutions" tabindex="-1">Common Proxy Configuration Issues and Solutions <a class="header-anchor" href="#common-proxy-configuration-issues-and-solutions" aria-label="Permalink to “Common Proxy Configuration Issues and Solutions”">​</a></h3><p>When configuring and using proxies, if you cannot use normally, you may encounter the following common issues:</p><h4 id="_1-connection-timeout" tabindex="-1">1. Connection Timeout <a class="header-anchor" href="#_1-connection-timeout" aria-label="Permalink to “1. Connection Timeout”">​</a></h4><p><strong>Problem Manifestation</strong>: After configuring proxy, long waiting occurs when requesting API.</p><p><strong>Solution</strong>:</p><ul><li>Check if proxy tool is running normally</li><li>Try changing routes</li><li>Confirm if proxy address and port are correct</li><li>Temporarily close firewall or security software, check if they are intercepting</li></ul><br><h4 id="_2-error-during-use" tabindex="-1">2. Error During Use <a class="header-anchor" href="#_2-error-during-use" aria-label="Permalink to “2. Error During Use”">​</a></h4><p><strong>Problem Manifestation</strong>: Error occurs when using large language model after configuring proxy</p><p><strong>Solution</strong>:</p><ul><li>If the proxy you configured listens to HTTPS protocol, such as <code>https://127.0.0.1:7890</code>, ensure the proxy server supports HTTPS requests.</li><li>Most local proxy tools (such as Clash, V2RayN, Shadowsocks clients, etc.) expect to receive HTTP or SOCKS protocol connections when listening to ports, not HTTPS connections.</li><li>Check if there are errors in the JSON format of the configuration file, including commas, quotes, etc.</li></ul><p>Configuring proxy is an important means to solve network access restrictions, especially for services like OpenAI. If the above methods cannot solve your problem, you can contact us for more detailed help.</p>`,73)])])}const P=o(d,[["render",u]]);export{E as __pageData,P as default};
